MAIN.PY:
import cv2
import pickle
from collections import defaultdict, deque
from models import load_models
from utils import cap, out
from tracking import process_frame

# Carregar os modelos usando a função load_models
model_detection, model_classification, extractor = load_models()

# Inicializar contadores
inside_count = 0
outside_count = 0
unique_ids_inside = set()
unique_ids_outside = set()
features_dict = {}
trajectories = {}  # Inicializar o dicionário de trajetórias

# Inicializar contadores de frames e classes
frame_counts = defaultdict(int)  # Conta o número de frames por objeto
class_counts = defaultdict(lambda: defaultdict(int))  # Conta a frequência de classes por objeto

# Definir as classes do modelo
classes = {0: "homem", 1: "mulher", 2: "naoidentificado"}

confirmed_ids = {}

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Chamar a função process_frame com o parâmetro trajectories
    inside_count, outside_count, unique_ids_inside, unique_ids_outside, features_dict, frame_counts, class_counts, confirmed_ids, trajectories = process_frame(
        frame, model_detection, model_classification, extractor, inside_count, outside_count, unique_ids_inside, unique_ids_outside, features_dict, classes, frame_counts, class_counts, confirmed_ids, trajectories
    )

    # Salvar o frame processado no arquivo de saída
    out.write(frame)

    # Mostrar o frame com as BBoxes e contagens
    cv2.imshow("Rastreamento", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Salvar características em disco
with open('features_dict.pkl', 'wb') as f:
    pickle.dump(features_dict, f)

# Liberar recursos
cap.release()
out.release()
cv2.destroyAllWindows()




TRACKING.PY
import cv2
from scipy.spatial.distance import cosine
from collections import defaultdict, deque
import time
import os
import numpy as np
from utils import crossed_line, line_points
from models import load_models

# Configurações
SIMILARITY_THRESHOLD = 0.9  # Ajuste o limiar de similaridade
FEATURE_TTL = 5400  # Tempo em segundos para manter as características
MAX_FEATURES_PER_ID = 20  # Número máximo de características armazenadas por ID
FRAMES_TO_CONFIRM = 15
SAVE_IMAGES = True  # Ativar/desativar salvamento de imagens
IMAGES_DIR = "id_images"  # Diretório para salvar as imagens por ID

# Criar diretório para salvar imagens, se não existir
if SAVE_IMAGES and not os.path.exists(IMAGES_DIR):
    os.makedirs(IMAGES_DIR)

# Configurações adicionais para análise de trajetória
TRAJECTORY_HISTORY_LENGTH = 40  # Número de posições anteriores a serem armazenadas
POSITION_THRESHOLD = 100  # Distância máxima (em pixels) para considerar uma correspondência de trajetória

def process_frame(frame, model_detection, model_classification, extractor, inside_count, outside_count, unique_ids_inside, unique_ids_outside, features_dict, classes, frame_counts, class_counts, confirmed_ids, trajectories):
    current_time = time.time()
    
    # Limpar características expiradas
    features_dict = {
        obj_id: (features_list, timestamp)
        for obj_id, (features_list, timestamp) in features_dict.items()
        if current_time - timestamp <= FEATURE_TTL
    }

    # Limpar trajetórias expiradas
    trajectories = {
        obj_id: traj for obj_id, traj in trajectories.items()
        if obj_id in features_dict  # Manter apenas trajetórias de IDs ativos
    }

    # Detecção inicial usando YOLO
    tracking_results = model_classification.track(frame, conf=0.65, persist=True, iou=0.6)

    for det in tracking_results[0].boxes:
        x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())
        confidence = det.conf[0].item()
        class_id = int(det.cls[0].item())
        class_name = classes.get(class_id, "desconhecido")
        center = ((x1 + x2) // 2, (y1 + y2) // 2)

        # Extrair características utilizando o modelo OSNet
        person_crop = frame[int(y1):int(y2), int(x1):int(x2)]
        features = extractor(person_crop).detach().cpu().numpy().flatten()

        # Verificar se a detecção corresponde a um ID já existente com base nas características
        matched_id = None
        max_similarity = 0
        for known_id, (known_features_list, _) in features_dict.items():
            avg_features_known = np.mean(known_features_list, axis=0)
            similarity = 1 - cosine(features, avg_features_known)
            if similarity > max_similarity and similarity > SIMILARITY_THRESHOLD:
                max_similarity = similarity
                matched_id = known_id

        # Se encontrou um ID correspondente, reutilizar o ID existente
        if matched_id:
            print(f"Detecção corresponde ao ID {matched_id} com similaridade {max_similarity:.2f}")
            obj_id = matched_id  # Reutilizar o ID existente

            # Adicionar as características atuais ao histórico do ID existente
            features_dict[obj_id][0].append(features)

            # Atualizar a trajetória do ID existente
            if obj_id in trajectories:
                trajectories[obj_id].append(center)
                if len(trajectories[obj_id]) > TRAJECTORY_HISTORY_LENGTH:
                    trajectories[obj_id].popleft()
            else:
                trajectories[obj_id] = deque([center], maxlen=TRAJECTORY_HISTORY_LENGTH)

            # Se o objeto já foi confirmado, usar o ID e a classe confirmados
            if obj_id in confirmed_ids:
                confirmed_data = confirmed_ids[obj_id]
                class_id = confirmed_data["class_id"]
                class_name = confirmed_data["class_name"]
            else:
                # Se ainda não foi confirmado, continuar contando os frames
                frame_counts[obj_id] += 1
                class_counts[obj_id][class_id] += 1

                # Se atingiu 15 frames, confirmar o ID
                if frame_counts[obj_id] == FRAMES_TO_CONFIRM:
                    predominant_class = max(class_counts[obj_id], key=class_counts[obj_id].get)
                    class_id = predominant_class
                    class_name = classes[predominant_class]
                    print(f"Objeto {obj_id} confirmado como {class_name} após {FRAMES_TO_CONFIRM} frames.")

                    # Armazenar o ID e a classe confirmados
                    confirmed_ids[obj_id] = {
                        "id": obj_id,
                        "class_id": class_id,
                        "class_name": class_name,
                        "confirmed": True  # Marcar como confirmado
                    }

                    # Só atualizar a contagem após a confirmação do ID e da classe
                    if obj_id not in unique_ids_inside and obj_id not in unique_ids_outside:
                        if crossed_line(center, line_points):
                            inside_count += 1
                            unique_ids_inside.add(obj_id)
                        else:
                            outside_count += 1
                            unique_ids_outside.add(obj_id)

        else:
            # Se não encontrou correspondência, verificar a trajetória
            trajectory_matched_id = None
            min_distance = float('inf')

            for known_id, traj in trajectories.items():
                if len(traj) < 2:
                    continue  # Não há trajetória suficiente para prever

                # Prever a próxima posição com base na trajetória
                last_pos = traj[-1]
                prev_pos = traj[-2]
                delta_x = last_pos[0] - prev_pos[0]
                delta_y = last_pos[1] - prev_pos[1]
                predicted_pos = (last_pos[0] + delta_x, last_pos[1] + delta_y)

                # Calcular a distância entre a posição atual e a posição prevista
                distance = np.sqrt((center[0] - predicted_pos[0]) ** 2 + (center[1] - predicted_pos[1]) ** 2)

                if distance < min_distance and distance < POSITION_THRESHOLD:
                    min_distance = distance
                    trajectory_matched_id = known_id

            # Se encontrou uma correspondência de trajetória, reutilizar o ID existente
            if trajectory_matched_id:
                print(f"Detecção corresponde ao ID {trajectory_matched_id} com base na trajetória (distância: {min_distance:.2f} pixels)")
                obj_id = trajectory_matched_id  # Reutilizar o ID existente

                # Adicionar as características atuais ao histórico do ID existente
                features_dict[obj_id][0].append(features)

                # Atualizar a trajetória do ID existente
                trajectories[obj_id].append(center)

                # Se o objeto já foi confirmado, usar o ID e a classe confirmados
                if obj_id in confirmed_ids:
                    confirmed_data = confirmed_ids[obj_id]
                    class_id = confirmed_data["class_id"]
                    class_name = confirmed_data["class_name"]
                else:
                    # Se ainda não foi confirmado, continuar contando os frames
                    frame_counts[obj_id] += 1
                    class_counts[obj_id][class_id] += 1

                    # Se atingiu 15 frames, confirmar o ID
                    if frame_counts[obj_id] == FRAMES_TO_CONFIRM:
                        predominant_class = max(class_counts[obj_id], key=class_counts[obj_id].get)
                        class_id = predominant_class
                        class_name = classes[predominant_class]
                        print(f"Objeto {obj_id} confirmado como {class_name} após {FRAMES_TO_CONFIRM} frames.")

                        # Armazenar o ID e a classe confirmados
                        confirmed_ids[obj_id] = {
                            "id": obj_id,
                            "class_id": class_id,
                            "class_name": class_name,
                            "confirmed": True  # Marcar como confirmado
                        }

                        # Só atualizar a contagem após a confirmação do ID e da classe
                        if obj_id not in unique_ids_inside and obj_id not in unique_ids_outside:
                            if crossed_line(center, line_points):
                                inside_count += 1
                                unique_ids_inside.add(obj_id)
                            else:
                                outside_count += 1
                                unique_ids_outside.add(obj_id)

            else:
                # Se não encontrou correspondência, criar um novo ID temporário
                obj_id = len(features_dict) + 1  # Gerar um novo ID temporário
                print(f"Nova detecção, criando ID temporário {obj_id}")

                # Armazenar as características para o novo ID temporário
                features_dict[obj_id] = (deque([features], maxlen=MAX_FEATURES_PER_ID), current_time)

                # Inicializar contadores para o novo ID temporário
                frame_counts[obj_id] = 1
                class_counts[obj_id] = defaultdict(int)
                class_counts[obj_id][class_id] += 1

                # Inicializar a trajetória para o novo ID temporário
                trajectories[obj_id] = deque([center], maxlen=TRAJECTORY_HISTORY_LENGTH)

                # Definir como "não identificado" até que seja confirmado
                class_id = 2  # Não identificado
                class_name = classes[2]

        # Salvar imagem do ID, se necessário
        if SAVE_IMAGES:
            id_dir = os.path.join(IMAGES_DIR, str(obj_id))
            if not os.path.exists(id_dir):
                os.makedirs(id_dir)
            image_path = os.path.join(id_dir, f"{int(current_time)}.jpg")
            cv2.imwrite(image_path, person_crop)

        # Atualizar contagem e IDs únicos (apenas para objetos confirmados)
        if obj_id in confirmed_ids:
            if obj_id not in unique_ids_inside and obj_id not in unique_ids_outside:
                if crossed_line(center, line_points):
                    inside_count += 1
                    unique_ids_inside.add(obj_id)
                else:
                    outside_count += 1
                    unique_ids_outside.add(obj_id)

            if obj_id not in unique_ids_inside and obj_id in unique_ids_outside:
                if crossed_line(center, line_points):
                    inside_count += 1
                    unique_ids_inside.add(obj_id)
                    outside_count -= 1
                    unique_ids_outside.remove(obj_id)

        # Ajustar cor da caixa com base na classe
        if class_name == "homem":
            color = (0, 255, 0)
        elif class_name == "mulher":
            color = (255, 0, 0)
        else:
            color = (0, 0, 255)

        # Desenhar BBox e a classe com confiança e ID
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame, f'ID: {obj_id} {class_name} {confidence:.2f}', 
                    (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

    # Desenhar a linha salva no frame
    for i in range(1, len(line_points)):
        cv2.line(frame, line_points[i-1], line_points[i], (0, 255, 0), 2)

    # Exibir as contagens
    cv2.putText(frame, f'Entraram: {inside_count}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.putText(frame, f'Fora: {outside_count}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

    return inside_count, outside_count, unique_ids_inside, unique_ids_outside, features_dict, frame_counts, class_counts, confirmed_ids, trajectories



    MODELS.PY
    from ultralytics import YOLO
import torchreid
import torch
import cv2
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_models():
    """
    Carrega os modelos YOLO para detecção e classificação, e o extrator de características OSNet.
    """
    # Verificar dispositivo (CPU ou GPU)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"Usando dispositivo: {device}")

    # Carregar os modelos YOLO
    logger.info("Carregando modelo YOLO para detecção...")
    model_detection = YOLO("yolov8m.pt").to(device)
    
    logger.info("Carregando modelo YOLO para classificação...")
    model_classification = YOLO(r"projeto_cam_ufcat\modelo_treinado_v8_Adam-310125-64batch.pt").to(device)

    # Inicializar o extrator de características do Torchreid (OSNet)
    logger.info("Carregando extrator de características OSNet...")
    extractor = torchreid.utils.FeatureExtractor(
        model_name='osnet_x1_0',
        model_path=r'projeto_cam_ufcat\osnet_x1_0_imagenet.pth',
        device=device
    )

    logger.info("Modelos carregados com sucesso!")
    return model_detection, model_classification, extractor


    UTILS.PY É O MESMO
    
