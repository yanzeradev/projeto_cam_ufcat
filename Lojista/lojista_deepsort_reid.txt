MAIN.PY:
import cv2
from models import load_models
from utils import cap, out
from tracking import process_frame

# Carregar modelos e tracker
model_detection, model_classification, tracker, extractor = load_models()

# Inicializar contadores
inside_count = 0
outside_count = 0
unique_ids_inside = set()
unique_ids_outside = set()
classes = {0: "homem", 1: "mulher", 2: "naoidentificado"}

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Processar frame
    inside_count, outside_count, unique_ids_inside, unique_ids_outside = process_frame(
        frame, model_detection, model_classification, tracker, extractor,
        inside_count, outside_count, unique_ids_inside, unique_ids_outside, classes
    )

    # Salvar e exibir frame
    out.write(frame)
    cv2.imshow("Rastreamento", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Liberar recursos
cap.release()
out.release()
cv2.destroyAllWindows()


TRACKING.PY:
import cv2
import time
import os
import numpy as np
from utils import crossed_line, line_points
from models import load_models

# Configurações
FRAMES_TO_CONFIRM = 20
SAVE_IMAGES = True
IMAGES_DIR = "id_images"
MAX_AGE = 40  # Tempo máximo (frames) que um tracker pode ficar sem atualização

def process_frame(frame, model_detection, model_classification, tracker, extractor,  # Adicione o extrator
                 inside_count, outside_count, unique_ids_inside, unique_ids_outside, classes):
    
    # Detecção inicial usando YOLO
    results = model_detection.predict(frame, conf=0.5, iou=0.7)
    
    # Preparar detecções para o DeepSORT com embeddings
    detections = []
    for det in results[0].boxes:
        x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())
        confidence = det.conf[0].item()
        class_id = int(det.cls[0].item())
        
        # Extrair região de interesse
        roi = frame[y1:y2, x1:x2]
        if roi.size == 0:
            continue
            
        # Gerar embedding com OSNet
        try:
            embedding = extractor(roi)[0]  # Extrai a feature vetor
        except Exception as e:
            print(f"Erro na extração de características: {e}")
            continue
            
        # Criar detecção no formato (bbox, confidence, class_id, embedding)
        detections.append(([x1, y1, x2 - x1, y2 - y1], confidence, class_id, embedding))

    # Atualizar tracker com as novas detecções (incluindo embeddings)
    tracks = tracker.update_tracks(detections, frame=frame)  # O DeepSort agora recebe as embeddings

    # Processar tracks atualizados
    for track in tracks:
        if not track.is_confirmed() or track.time_since_update > 0:
            continue

        # Obter coordenadas da bounding box
        ltrb = track.to_ltrb()
        x1, y1, x2, y2 = map(int, ltrb)
        center = ((x1 + x2) // 2, (y1 + y2) // 2)
        obj_id = track.track_id
        class_id = track.get_det_class()
        class_name = classes.get(class_id, "desconhecido")

        # Atualizar contagem
        if obj_id not in unique_ids_inside and obj_id not in unique_ids_outside:
            if crossed_line(center, line_points):
                inside_count += 1
                unique_ids_inside.add(obj_id)
            else:
                outside_count += 1
                unique_ids_outside.add(obj_id)

        # Ajustar cor da caixa com base na classe
        color = (0, 255, 0) if class_name == "homem" else (255, 0, 0) if class_name == "mulher" else (0, 0, 255)

        # Desenhar elementos
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame, f'ID: {obj_id} {class_name}', (x1, y1 - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

        # Salvar imagem do ID (opcional)
        if SAVE_IMAGES:
            save_id_image(frame, obj_id, x1, y1, x2, y2)

    # Desenhar a linha de contagem
    for i in range(1, len(line_points)):
        cv2.line(frame, line_points[i-1], line_points[i], (0, 255, 0), 2)

    # Exibir contagens
    cv2.putText(frame, f'Entraram: {inside_count}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.putText(frame, f'Fora: {outside_count}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

    return inside_count, outside_count, unique_ids_inside, unique_ids_outside

def save_id_image(frame, obj_id, x1, y1, x2, y2):
    """Salva imagens dos IDs para verificação manual"""
    id_dir = os.path.join(IMAGES_DIR, str(obj_id))
    if not os.path.exists(id_dir):
        os.makedirs(id_dir)
    person_crop = frame[y1:y2, x1:x2]
    if person_crop.size > 0:
        cv2.imwrite(os.path.join(id_dir, f"{int(time.time())}.jpg"), person_crop)

MODELS.PY:
from ultralytics import YOLO
import torch
import cv2
import logging
from deep_sort_realtime.deepsort_tracker import DeepSort
import torchreid

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_models():
    """
    Carrega os modelos YOLO para detecção e classificação, e o tracker DeepSORT.
    """
    # Verificar dispositivo (CPU ou GPU)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"Usando dispositivo: {device}")

    # Carregar os modelos YOLO
    logger.info("Carregando modelo YOLO para detecção...")
    model_detection = YOLO("yolov8m.pt").to(device)
    
    logger.info("Carregando modelo YOLO para classificação...")
    model_classification = YOLO(r"projeto_cam_ufcat\modelo_treinado_v8_Adam-310125-64batch.pt").to(device)

    # Inicializar o tracker DeepSORT
    logger.info("Inicializando tracker DeepSORT...")
    tracker = DeepSort(
        max_age=50,            # Quantos frames um objeto é mantido antes de ser esquecido
        n_init=20,              # Frames necessários para confirmar um objeto
        nms_max_overlap=0.5,   # Usa GPU para extração de características
        nn_budget=100          # Memória do modelo de ReID
    )

    logger.info("Carregando extrator de características OSNet...")
    extractor = torchreid.utils.FeatureExtractor(
        model_name='osnet_x1_0',
        model_path=r'projeto_cam_ufcat\osnet_x1_0_imagenet.pth',
        device=device
    )

    logger.info("Modelos carregados com sucesso!")
    return model_detection, model_classification, tracker, extractor

UTILS.PY É O MESMO DE ANTES
